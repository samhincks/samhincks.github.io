---
layout: post
title:  "Making Music with Binary Inputs"
date:   2016-12-15 14:02:36 -0400
categories: music brain 
---
I think everybody can make music, and I'm starting a blog series explaining how and why everyone should take advantage of this recent affordance of technology. The only intellectual requirement is a capacity to supply binary (yes-no) inputs to a computer as well as an ear that knows what it likes.

 Music is an informational sweetspot between sound and brain. It consists of patterns that lure the brain into dedicated prediction cycles, with enough repetition in the signal to tempt auditory predictors but enough variation so that some error spreads, causing data and computation to propogate among the brain's networks. Presumably, the search for resolution of that remaining auditory signal activates networks computing states with valence or emotions, since a resembling soundscape or sequence of notes had previously been encountered, and that previous encounter had an emotional character to it (a concert, a party, times with friends, childhood, a scene from a movie). 

 Some small percentage of humans suffer amusia but everybody else enjoys music, which is not surprising if one takes this Bayesian account for musical experiences seriously. In that framework, the phenomenon of experience is proportional to the amount of interest our brain pays to external inputs and the degree to which that interest or prediction is mistaken. For everyone capable of absorbing musical patterns and enlisting these probability distrubutions for prediction, music induces experience since experience is equivalent to the computational events that occur when engaged brain regions correct their errors.

 However, most people are not entirely exploiting the full potential of music since they have not mastered how to use an instrument or how to create an original song, and this is not their fault because most analogue instruments have poor user interfaces. It is also not the fault of the makers of analogue instruments since an analogue instrument must exploit some property of physics, and the maneuveurs that a player can make with that instrument must conform to whatever enables the instruments sound production. For instance, a guitar boils down to the pace at which strings vibrate; it has frets strategically placed so that a properly tuned guitar outputs waveforms which music has decided to be its constituent atoms or legal notes when the user places her hand on that fret and plucks the string. When I try to do this, my hand feels uncomfortable and most of the sounds I make on a guitar hurt my ear because my motor memory has not absorbed good models for what constitutes legal transitions. I also don't feel any inclination to master this impressive but archaic sound producer given the availability of software instruments.

 Since computers can output any sound according to a well formulated rule, the challenge of a software instrument is to expose a set of manipulations to a predefined soundscape which cause variation to the signal but always keeps the signal within the same family of sound. I am new to synthesized music, but it appears to me that the interesting component of making original music now is in the design of new software instruments. In software, the most important criteria for a good user interface is that the user is forbidden from making mistakes. With Garageband and its upgraded version Logic Pro, Apple appears to have figured out a way to constrain certain instruments so that every input I provide maintains the musical legality of the sound.

 It took me three sessions of using Garageband until I produced a song that I liked. I had no background in music and did not consult any tutorials. After three months of investigation (4-8 hours a day), I reached the milestone of producing a song which looped in my mind, so that I had the repeated urge listen to it. Part of this fixation was mildly narcissistic but I think mostly I had figured out my own brain well enough to know how to layer a set of patterns to induce rich states. 

In this blog series, I will attempt to teach people with zero musical background how to optimize soundscapes for their own brain. Making music in computer software is a powerful antidepressant which has only recently been democratized. 

At this point, I've compiled ~100 songs. Most of them are not worth fighting for since they wouldn't stand the more rigorous criticism of another person's brain. As of December 15 2016, I've made 6 songs that I would consider optimizing and placing in an album. 

I have approached each song as an experiment testing a possible hack about how to fit sound for my brain. This approach works well for me since it means I initiate each session with curiosity and a willingness to fail. In either case, the outcome is a success because if the song failed, then I have dismissed some poor method of making music. The risk with this method is that it's so engaging that I hardly want to do anything else. I am addicted to making music. And making music is not a worthwhile profession, since (as I have learned) computer's will soon only need one input from the human (I like this vs. I don't like this).

Stay tuned to this series to learn about my experiments, how to frame music making as an experiment, and how to capitalize on modern technologies for easy flow states. 

